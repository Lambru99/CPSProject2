{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Monta Google Drive per accedere ai file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Importa le librerie necessarie\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
        "\n",
        "# Carica il dataset da Google Drive\n",
        "file_path = \"/content/drive/MyDrive/cyber/definitivo.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rimuovi spazi bianchi dai nomi delle colonne\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Converti la colonna 'Normal/Attack' in numeri\n",
        "df['Normal/Attack'] = df['Normal/Attack'].map({'Normal': 0, 'Attack': 1})\n",
        "\n",
        "# Rimuovi la colonna 'Timestamp' poiché non è necessaria per il rilevamento delle anomalie\n",
        "df = df.drop(columns=['Timestamp'])\n",
        "\n",
        "# Rimuovi tutte le righe che contengono almeno un valore NaN\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Dividi il dataset in features (X) e target (y)\n",
        "X = df.drop(columns=['Normal/Attack'])\n",
        "y = df['Normal/Attack']\n",
        "\n",
        "# Standardizza le features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Dividi il dataset in training set e test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Reshape dei dati di input per includere le informazioni temporali\n",
        "X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test_reshaped = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Crea il modello\n",
        "def create_lstm_model(input_shape):\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.LSTM(64, input_shape=input_shape, return_sequences=True),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.LSTM(32, return_sequences=False),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "# Imposta la forma dei dati correttamente per l'input del modello LSTM\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "\n",
        "# Crea il modello\n",
        "model = create_lstm_model(input_shape)\n",
        "\n",
        "# Fai il fit del modello\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Valuta il modello sul set di test\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fV4_4AyCDdR2",
        "outputId": "ea50ed4d-6793-419a-c398-e5ecfa353670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Effettua previsioni sul set di test\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Imposta una soglia per classificare le previsioni come anomalie\n",
        "threshold = 0.5\n",
        "# Confronta le previsioni con la soglia per identificare gli esempi anomali\n",
        "y_pred_anomalies = (y_pred > threshold).astype(int)"
      ],
      "metadata": {
        "id": "MzBJKn5bRlGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Analyzing the model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NOHuzR0pr_1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score"
      ],
      "metadata": {
        "id": "aSD8n9PvzT6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Report\n",
        "\n",
        "\n",
        "**Precision**: Represents the proportion of instances classified as positive that are actually positive. In other words, it is the model's ability to not mislabel negative instances as positive. For class 0 (non-anomaly), precision is 100%, meaning all instances classified as non-anomalies are actually non-anomalies. For class 1 (anomaly), precision is 98%, indicating that 2% of instances classified as anomalies might be false positives.\n",
        "\n",
        "**Recall**: Represents the proportion of total positive instances that were correctly identified by the model. In other words, it is the model's ability to find all positive instances. For class 0, recall is 100%, indicating that all non-anomaly instances were correctly identified. For class 1, recall is 95%, meaning that 5% of anomaly instances might not have been detected by the model.\n",
        "\n",
        "**F1-score**: It is a weighted average of precision and recall. It represents the balance between precision and recall. A high F1-score indicates a good balance between precision and recall.\n",
        "\n",
        "**Support**: Represents the number of instances in each class in the test dataset.\n",
        "\n",
        "**Accuracy**: Represents the proportion of correct predictions out of the total predictions made by the model. In your case, accuracy is 100%, meaning all predictions made by the model are correct in the test dataset."
      ],
      "metadata": {
        "id": "mVyRgI-Gyp6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Valuta le prestazioni del modello\n",
        "print(classification_report(y_test, y_pred_anomalies))"
      ],
      "metadata": {
        "id": "7cUEvMcYyWCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Curves\n",
        "The learning curve can provide valuable insights into the performance of a machine learning model during training.\n",
        "As we can see from these two learning curves that analyze both loss and accuracy, the model seems to be learning effectively and generalizing well to unseen data.\n",
        "\n",
        "* Both the training and validation loss curves show a clear downward trend, indicating that the model is consistently improving its ability to correctly classify instances.\n",
        "* The gap between the training and validation curves remains small throughout the training process, suggesting that the model is not significantly overfitting the training data. This is crucial for good generalization to unseen data.\n",
        "* Both training and validation accuracy curves show an upward trend, reaching high final values. This implies that the model is correctly classifying both training and unseen data with a high degree of accuracy."
      ],
      "metadata": {
        "id": "PEZoB8bfzIhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Learning Curve: Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Learning Curve: Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OPmH7WHUx4dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix helps visualize true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "As we already discussed for the curves, this confusion matrix further strengthens the case that the model is a highly effective anomaly detector.\n",
        "\n",
        "* Just like the low training and validation loss in the curves, the TPR of 0.98 in the confusion matrix confirms the model's ability to correctly identify real attacks. This signifies minimal instances slipping through the cracks.\n",
        "* The TNR of 1.00 echoes the small gap between curves, indicating excellent generalization. The model rarely misclassifies normal instances as attacks, minimizing false alarms.\n",
        "* The precision of 0.98 for attacks reiterates the curves' suggestion of minimal overfitting. The model focuses on true positives, reducing noise and false positives.\n",
        "* The recall of 0.98 for attacks aligns with the high validation accuracy, reflecting the model's ability to capture most attacks. Missed attacks are rare.\n",
        "* With only 387 false positives, the confusion matrix emphasizes the curves' indication of minimal overfitting."
      ],
      "metadata": {
        "id": "7Ryp8yLayHDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred_anomalies)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fuz2-ubnr8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC curve illustrates the trade-off between sensitivity and specificity at various threshold settings. Based on the combination of all the analyses, it's likely that your model slightly prioritizes sensitivity, especially at stricter thresholds. However, it maintains a good overall balance between sensitivity and specificity across various thresholds.\n",
        "\n"
      ],
      "metadata": {
        "id": "PDrYrXA72S5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting ROC curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xAo4ecS9xGYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The precision-recall curve illustrates the trade-off between precision and recall. As we can see the curve starts very close to the top (representing perfect precision) and gradually decreases as recall increases. This means the model can achieve very high precision even when only a small portion of actual attacks are identified.\n",
        "\n",
        "The decrease in precision as recall increases is relatively slow, indicating that the model maintains good precision even as it identifies more attacks. This is a positive sign, as it suggests the model does not generate a large number of false positives when trying to catch more attacks.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tcZERRsXxZFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Precision-Recall curve\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred)\n",
        "average_precision = average_precision_score(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.step(recall, precision, color='b', alpha=0.2, where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('Precision-Recall curve: AP={0:0.2f}'.format(average_precision))\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FUCHvbHqxRWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram of predicted anomaly scores shows the distribution of anomaly scores across the test set. Basing on this histogram it seems like this model's predictions are indeed slightly skewed towards the attack class (higher values), consistent with what I mentioned earlier. Here's a more detailed analysis:\n",
        "\n",
        "**Observations**:\n",
        "\n",
        "* **Shape**: The histogram has a relatively smooth, unimodal distribution, as expected for a well-performing model.\n",
        "* **Distribution**: The peak of the distribution leans slightly towards the right side, indicating that a larger portion of predictions fall into higher values, which represent predicted attacks. This aligns with the idea that the model prioritizes sensitivity to some extent, aiming to catch most attacks even if it means some false positives.\n",
        "* **Outliers**: There are a few outliers on both ends of the histogram, but they are relatively infrequent. This suggests that most predictions are confident (closer to either 0 or 1), while a small number fall into the less confident range."
      ],
      "metadata": {
        "id": "JU9s6y0u4grI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Histogram of Predicted Anomaly Scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_pred, bins=50, color='blue', alpha=0.7)\n",
        "plt.xlabel('Predicted Anomaly Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Predicted Anomaly Scores')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QjJIKM7jxeS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attack model"
      ],
      "metadata": {
        "id": "_EjLKbafn-YD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "It's important to clarify that the DAICS paper itself doesn't propose attacks against the anomaly detection model. Its focus is on improving the detection of actual anomalies, not exploring ways to bypass it.\n",
        "\n",
        "However, the field of anomaly detection does consider potential attacks and vulnerabilities, so it's valuable to discuss them even if not directly addressed in the specific paper. Here are some general attack types relevant to anomaly detection models like DAICS:\n",
        "\n",
        "Adversarial attacks: These involve crafting specific inputs that intentionally trigger false positives or negatives. In the context of ICSs, this could involve manipulating sensor data to appear normal while hiding malicious activity.\n",
        "\n",
        "Data poisoning: Attackers inject malicious data into the training dataset to manipulate the model's behavior. This could be difficult for DAICS since it requires few data samples, but not impossible.\n",
        "\n",
        "Model inversion: Attackers try to recover sensitive information from the model itself, potentially revealing normal behavior patterns and making it easier to craft attacks.\n",
        "\n",
        "Evasion attacks: Attackers modify their actions to avoid detection by the model while still achieving their goals. This could involve mimicking normal behavior or exploiting specific weaknesses in the model's decision-making process.\n",
        "\n",
        "Synergy attacks: Attackers combine multiple techniques, like data poisoning and evasion, to make their attacks more effective.\n",
        "\n",
        "It's important to note that DAICS does incorporate some measures to mitigate these vulnerabilities, such as its focus on learning changes in behavior and its robustness to noise. However, no model is completely immune to attack, and ongoing research is crucial to staying ahead of potential threats.\n",
        "\n",
        "If you're interested in learning more about specific attacks against anomaly detection models or the defense mechanisms employed by DAICS, feel free to ask!"
      ],
      "metadata": {
        "id": "pZQJm5DfoJFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate synthetic attacks by adding outliers to the test set\n",
        "def add_outliers(X_test, outlier_ratio=1):\n",
        "    num_outliers = int(len(X_test) * outlier_ratio)\n",
        "    indices = np.random.choice(len(X_test), num_outliers, replace=False)\n",
        "    X_test_outliers = X_test.copy()\n",
        "    for idx in indices:\n",
        "        # Add outliers by multiplying random noise to a random feature\n",
        "        feature_idx = np.random.randint(0, X_test.shape[1])\n",
        "        X_test_outliers[idx, feature_idx] *= np.random.uniform(5, 10)  # Add outlier by scaling\n",
        "    return X_test_outliers\n",
        "\n",
        "# Simulate attacks by adding outliers to the test set\n",
        "X_test_attacked = add_outliers(X_test)\n",
        "\n",
        "# Evaluate the model on attacked test set\n",
        "test_loss_attacked, test_accuracy_attacked = model.evaluate(X_test_attacked, y_test)\n",
        "print(\"Test Accuracy on Attacked Test Set:\", test_accuracy_attacked)\n"
      ],
      "metadata": {
        "id": "nFv9KaYYoBaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate synthetic attacks by adding malicious instances to the training set\n",
        "def add_malicious_instances(X_train, y_train, malicious_ratio=1):\n",
        "    num_malicious = int(len(X_train) * malicious_ratio)\n",
        "    malicious_instances = np.random.rand(num_malicious, X_train.shape[1])  # Generate random malicious instances\n",
        "    malicious_labels = np.ones(num_malicious)  # Assign malicious labels\n",
        "    X_train_malicious = np.concatenate([X_train, malicious_instances])\n",
        "    y_train_malicious = np.concatenate([y_train, malicious_labels])\n",
        "    return X_train_malicious, y_train_malicious\n",
        "\n",
        "# Simulate a data poisoning attack by adding malicious instances to the training set\n",
        "X_train_poisoned, y_train_poisoned = add_malicious_instances(X_train, y_train)\n",
        "\n",
        "# Train the model on the poisoned training set\n",
        "poisoned_model = create_lstm_model(input_shape)\n",
        "history_poisoned = poisoned_model.fit(X_train_poisoned, y_train_poisoned, epochs=1, batch_size=128, validation_split=0.2)\n",
        "\n",
        "# Evaluate the poisoned model on the original test set\n",
        "test_loss_poisoned, test_accuracy_poisoned = poisoned_model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy of Poisoned Model on Original Test Set:\", test_accuracy_poisoned)\n"
      ],
      "metadata": {
        "id": "MsWuwvx3qTaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to generate adversarial examples using FGSM with NumPy\n",
        "def fgsm_attack_numpy(model, X, y, epsilon=0.5):\n",
        "    # Compute gradients of the loss with respect to the input data\n",
        "    gradient = compute_gradient(model, X, y)\n",
        "\n",
        "    # Compute the sign of the gradient\n",
        "    sign_gradient = np.sign(gradient)\n",
        "\n",
        "    # Generate perturbation\n",
        "    perturbation = epsilon * sign_gradient\n",
        "\n",
        "    # Create adversarial examples\n",
        "    adversarial_X = X + perturbation.T  # Transpose perturbation to match the shape of X\n",
        "\n",
        "    return adversarial_X\n",
        "\n",
        "# Function to compute gradient of the loss with respect to the input data\n",
        "def compute_gradient(model, X, y):\n",
        "    # Forward pass\n",
        "    logits = model.predict(X)\n",
        "\n",
        "    # Convert y to NumPy array and reshape it\n",
        "    y_array = np.array(y).reshape(-1, 1)\n",
        "\n",
        "    # Compute loss gradient manually (assuming binary cross-entropy loss)\n",
        "    error = logits - y_array\n",
        "    gradient = X.T @ error / len(X)\n",
        "\n",
        "    return gradient\n",
        "\n",
        "# Generate adversarial examples using FGSM with NumPy\n",
        "epsilon = 0.1  # Magnitude of perturbation\n",
        "X_test_adv_numpy = fgsm_attack_numpy(model, X_test, np.array(y_test), epsilon)\n",
        "\n",
        "# Evaluate the model on the adversarial examples\n",
        "test_loss_adv_numpy, test_accuracy_adv_numpy = model.evaluate(X_test_adv_numpy, y_test)\n",
        "print(\"Test Accuracy on Adversarial Examples (NumPy):\", test_accuracy_adv_numpy)\n"
      ],
      "metadata": {
        "id": "GY6JeP2JsY6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Importa le librerie necessarie\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crea il modello autoencoder LSTM\n",
        "class LSTM_Autoencoder:\n",
        "    def __init__(self, input_shape):\n",
        "        self.input_shape = input_shape\n",
        "        self.encoder = self.build_encoder()\n",
        "        self.decoder = self.build_decoder()\n",
        "        self.autoencoder = self.build_autoencoder()\n",
        "\n",
        "    def build_encoder(self):\n",
        "        encoder = Sequential([\n",
        "            LSTM(32, input_shape=self.input_shape, return_sequences=True),\n",
        "            LSTM(16, return_sequences=False)\n",
        "        ])\n",
        "        return encoder\n",
        "\n",
        "    def build_decoder(self):\n",
        "        decoder = Sequential([\n",
        "            RepeatVector(self.input_shape[0]),\n",
        "            LSTM(16, return_sequences=True),\n",
        "            LSTM(32, return_sequences=True),\n",
        "            TimeDistributed(Dense(self.input_shape[1]))\n",
        "        ])\n",
        "        return decoder\n",
        "\n",
        "    def build_autoencoder(self):\n",
        "        autoencoder = Sequential([self.encoder, self.decoder])\n",
        "        autoencoder.compile(optimizer='adam', loss='mse')\n",
        "        return autoencoder\n",
        "\n",
        "    def train(self, X_train, epochs=10, batch_size=128, validation_split=0.2):\n",
        "        history = self.autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split)\n",
        "        return history\n",
        "\n",
        "    def encode(self, X):\n",
        "        encoded_data = self.encoder.predict(X)\n",
        "        return encoded_data\n",
        "\n",
        "    def decode(self, encoded_data):\n",
        "        decoded_data = self.decoder.predict(encoded_data)\n",
        "        return decoded_data\n",
        "\n",
        "# Imposta la forma dei dati correttamente per l'input dell'autoencoder LSTM\n",
        "input_shape_autoencoder = (X_train.shape[1], 1)\n",
        "\n",
        "# Crea il modello autoencoder\n",
        "autoencoder_model = LSTM_Autoencoder(input_shape_autoencoder)\n",
        "\n",
        "# Fai il fit dell'autoencoder\n",
        "autoencoder_history = autoencoder_model.train(X_train_reshaped)\n",
        "\n",
        "# Usa l'autoencoder per ottenere le rappresentazioni latenti\n",
        "encoded_train = autoencoder_model.encode(X_train_reshaped)\n",
        "encoded_test = autoencoder_model.encode(X_test_reshaped)\n",
        "\n",
        "# Successivamente, potresti utilizzare le rappresentazioni latenti come input per un'altra rete neurale per la classificazione o il rilevamento di anomalie.\n"
      ],
      "metadata": {
        "id": "mZi50jIlTqsk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}